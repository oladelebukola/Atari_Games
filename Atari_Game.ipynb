{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9kmpex42rUY",
        "outputId": "bdf2fe13-ac15-431c-dfc9-0a4fb2e774f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.56.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.13)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.7.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
            "Collecting gym==0.19.0\n",
            "  Using cached gym-0.19.0.tar.gz (1.6 MB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Collecting gymnasium[atari]\n",
            "  Downloading gymnasium-0.29.0-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.8/953.8 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (4.7.1)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[atari])\n",
            "  Using cached Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting shimmy[atari]<1.0,>=0.1.0 (from gymnasium[atari])\n",
            "  Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0->gymnasium[atari])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium[atari]) (6.0.0)\n",
            "Installing collected packages: farama-notifications, gymnasium, ale-py, shimmy\n",
            "  Attempting uninstall: ale-py\n",
            "    Found existing installation: ale-py 0.7.5\n",
            "    Uninstalling ale-py-0.7.5:\n",
            "      Successfully uninstalled ale-py-0.7.5\n",
            "Successfully installed ale-py-0.8.1 farama-notifications-0.0.4 gymnasium-0.29.0 shimmy-0.2.1\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "/bin/bash: line 1: pp: command not found\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: retro in /usr/local/lib/python3.10/dist-packages (2.9.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: gym[classic_control] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[classic_control]) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[classic_control]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[classic_control]) (0.0.8)\n",
            "Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.10/dist-packages (from gym[classic_control]) (2.1.0)\n",
            "Requirement already satisfied: keras-rl in /usr/local/lib/python3.10/dist-packages (0.4.2)\n",
            "Requirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.10/dist-packages (from keras-rl) (2.12.0)\n",
            "Requirement already satisfied: gym[accept-rom-license] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license]) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license]) (0.0.8)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license]) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (8.1.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (4.65.0)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (0.6.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install tensorflow\n",
        "!pip install --upgrade gym==0.19.0\n",
        "!pip install gymnasium[atari]\n",
        "!pip install keras\n",
        "!pp install keras-r12\n",
        "!pip install gym retro\n",
        "!pip install gym[classic_control]\n",
        "!pip install keras-rl\n",
        "!pip install gym[accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ale-py\n",
        "!pip install retro"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NfIL51sA85M",
        "outputId": "f790d54c-3da7-4e4f-e32b-188a71828225"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ale-py in /usr/local/lib/python3.10/dist-packages (0.7.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ale-py) (1.22.4)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py) (6.0.0)\n",
            "Requirement already satisfied: retro in /usr/local/lib/python3.10/dist-packages (2.9.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DQNAgent TRAINING ON CARTPOLE\n"
      ],
      "metadata": {
        "id": "vWIK9LSqciEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from rl.agents import DQNAgent\n",
        "from rl.policy import BoltzmannQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "2xSZBYZ3a38P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DQN:\n",
        "    def __init__(self, state_space, action_space):\n",
        "        self.state_space = state_space\n",
        "        self.action_space = action_space\n",
        "        self.memory = []\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.epsilon_min = 0.01\n",
        "        self.gamma = 0.99\n",
        "        self.learning_rate = 0.001\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(24, input_shape=self.state_space, activation='relu'))\n",
        "        model.add(Dense(24, activation='relu'))\n",
        "        model.add(Dense(self.action_space, activation='linear'))\n",
        "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.choice(self.action_space)\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values[0])\n"
      ],
      "metadata": {
        "id": "79hAi3EZa4eV"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the environment\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "state_space = env.observation_space.shape\n",
        "action_space = env.action_space.n\n",
        "\n",
        "agent = DQN(state_space, action_space)"
      ],
      "metadata": {
        "id": "Ifsgj1iwa5AG"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_env(env,agent,episodes=10,batch_size = 32):\n",
        "  for episode in range(episodes):\n",
        "      state = env.reset()\n",
        "      done = False\n",
        "      total_reward = 0\n",
        "\n",
        "      while not done:\n",
        "          # Preprocess state (if necessary)\n",
        "          # (No need for np.expand_dims)\n",
        "\n",
        "          # Choose action using epsilon-greedy policy\n",
        "          if np.random.rand() < agent.epsilon:\n",
        "              action = env.action_space.sample()  # Explore: choose a random action\n",
        "          else:\n",
        "              state_input = np.expand_dims(state, axis=0)  # Adding batch dimension\n",
        "              # action = np.argmax(agent.model.predict(state_input))\n",
        "              action = tf.convert_to_tensor(action) # Exploit: choose the best action\n",
        "\n",
        "          # action= self.env.action_space.sample()  # selects an action from a sample space randomly\n",
        "          # obs, reward, done, *info = self.env.step(action)\n",
        "\n",
        "\n",
        "          # Take the action in the environment\n",
        "          action= env.action_space.sample()\n",
        "          next_state, reward, done, *info = env.step(action)\n",
        "\n",
        "          # Preprocess next_state (if necessary)\n",
        "          # (No need for np.expand_dims)\n",
        "\n",
        "          # Store the experience in memory\n",
        "          agent.remember(state, action, reward, next_state, done)\n",
        "\n",
        "          # Update the state\n",
        "          state = next_state\n",
        "          total_reward += reward\n",
        "\n",
        "          # Apply epsilon decay\n",
        "          if agent.epsilon > agent.epsilon_min:\n",
        "              agent.epsilon *= agent.epsilon_decay\n",
        "\n",
        "          # Clear the previous plot and display the current frame\n",
        "          # clear_output(wait=True)\n",
        "          # plt.imshow(env.render())\n",
        "          # plt.show()\n",
        "\n",
        "          # Print episode statistics\n",
        "      print(f\"Episode: {episode+1}, Total Reward: {total_reward}, Epsilon: {agent.epsilon}\")\n",
        "\n",
        "# Close the environment after training\n",
        "env.close()"
      ],
      "metadata": {
        "id": "Ohe8J4ZPb7ti"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the environment\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "state_space = env.observation_space.shape\n",
        "action_space = env.action_space.n\n",
        "\n",
        "agent = DQN(state_space, action_space)\n",
        "train_env(env,agent,episodes=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAYlLP-wfeCM",
        "outputId": "9367d4ad-8f68-4be1-d98b-253d5eab706c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 1, Total Reward: 10.0, Epsilon: 0.9511101304657719\n",
            "Episode: 2, Total Reward: 17.0, Epsilon: 0.8734200960253871\n",
            "Episode: 3, Total Reward: 17.0, Epsilon: 0.8020760579717637\n",
            "Episode: 4, Total Reward: 13.0, Epsilon: 0.7514768435208588\n",
            "Episode: 5, Total Reward: 16.0, Epsilon: 0.6935613678313175\n",
            "Episode: 6, Total Reward: 24.0, Epsilon: 0.6149486215357263\n",
            "Episode: 7, Total Reward: 14.0, Epsilon: 0.5732736268885887\n",
            "Episode: 8, Total Reward: 30.0, Epsilon: 0.4932355662165453\n",
            "Episode: 9, Total Reward: 24.0, Epsilon: 0.43732904629000013\n",
            "Episode: 10, Total Reward: 17.0, Epsilon: 0.4016064652978155\n",
            "Episode: 11, Total Reward: 23.0, Epsilon: 0.3578751580867638\n",
            "Episode: 12, Total Reward: 22.0, Epsilon: 0.32050833588933575\n",
            "Episode: 13, Total Reward: 15.0, Epsilon: 0.29729358661854943\n",
            "Episode: 14, Total Reward: 23.0, Epsilon: 0.2649210072611673\n",
            "Episode: 15, Total Reward: 27.0, Epsilon: 0.231387331601191\n",
            "Episode: 16, Total Reward: 23.0, Epsilon: 0.20619134658263935\n",
            "Episode: 17, Total Reward: 10.0, Epsilon: 0.19611067854912728\n",
            "Episode: 18, Total Reward: 9.0, Epsilon: 0.18746015382974018\n",
            "Episode: 19, Total Reward: 18.0, Epsilon: 0.1712870076899825\n",
            "Episode: 20, Total Reward: 21.0, Epsilon: 0.15417328217978102\n",
            "Episode: 21, Total Reward: 16.0, Epsilon: 0.14229132060898236\n",
            "Episode: 22, Total Reward: 19.0, Epsilon: 0.12936504510050365\n",
            "Episode: 23, Total Reward: 49.0, Epsilon: 0.10119240105273684\n",
            "Episode: 24, Total Reward: 12.0, Epsilon: 0.09528507271768329\n",
            "Episode: 25, Total Reward: 23.0, Epsilon: 0.08490939117940327\n",
            "Episode: 26, Total Reward: 33.0, Epsilon: 0.07196434741762824\n",
            "Episode: 27, Total Reward: 19.0, Epsilon: 0.06542683706543727\n",
            "Episode: 28, Total Reward: 14.0, Epsilon: 0.0609928681304848\n",
            "Episode: 29, Total Reward: 19.0, Epsilon: 0.0554520479727078\n",
            "Episode: 30, Total Reward: 23.0, Epsilon: 0.0494138221100385\n",
            "Episode: 31, Total Reward: 18.0, Epsilon: 0.0451506389749415\n",
            "Episode: 32, Total Reward: 23.0, Epsilon: 0.04023414326483323\n",
            "Episode: 33, Total Reward: 53.0, Epsilon: 0.030847362533106718\n",
            "Episode: 34, Total Reward: 18.0, Epsilon: 0.028186002814352063\n",
            "Episode: 35, Total Reward: 33.0, Epsilon: 0.023888845163905856\n",
            "Episode: 36, Total Reward: 27.0, Epsilon: 0.020864997436994256\n",
            "Episode: 37, Total Reward: 31.0, Epsilon: 0.01786215438933485\n",
            "Episode: 38, Total Reward: 12.0, Epsilon: 0.01681941195362342\n",
            "Episode: 39, Total Reward: 76.0, Epsilon: 0.011491188384178622\n",
            "Episode: 40, Total Reward: 18.0, Epsilon: 0.010499784796482848\n",
            "Episode: 41, Total Reward: 15.0, Epsilon: 0.00998645168764533\n",
            "Episode: 42, Total Reward: 21.0, Epsilon: 0.00998645168764533\n",
            "Episode: 43, Total Reward: 44.0, Epsilon: 0.00998645168764533\n",
            "Episode: 44, Total Reward: 21.0, Epsilon: 0.00998645168764533\n",
            "Episode: 45, Total Reward: 15.0, Epsilon: 0.00998645168764533\n",
            "Episode: 46, Total Reward: 38.0, Epsilon: 0.00998645168764533\n",
            "Episode: 47, Total Reward: 16.0, Epsilon: 0.00998645168764533\n",
            "Episode: 48, Total Reward: 19.0, Epsilon: 0.00998645168764533\n",
            "Episode: 49, Total Reward: 55.0, Epsilon: 0.00998645168764533\n",
            "Episode: 50, Total Reward: 21.0, Epsilon: 0.00998645168764533\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DQNAgent TRAINING ON SPACE INVADERS"
      ],
      "metadata": {
        "id": "r7OhBAyg4iFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from rl.agents import DQNAgent\n",
        "from rl.policy import BoltzmannQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "CQU3IF3u4wf0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DQN:\n",
        "    def __init__(self, state_space, action_space):\n",
        "        self.state_space = state_space\n",
        "        self.action_space = action_space\n",
        "        self.memory = []\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.epsilon_min = 0.01\n",
        "        self.gamma = 0.99\n",
        "        self.learning_rate = 0.001\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(24, input_shape=self.state_space, activation='relu'))\n",
        "        model.add(Dense(24, activation='relu'))\n",
        "        model.add(Dense(self.action_space, activation='linear'))\n",
        "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.choice(self.action_space)\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "# Initialize DQN agent\n",
        "agent = DQN(state_space, action_space)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_tace_WA4-rL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create the Space Invaders environment\n",
        "env = gym.make('SpaceInvaders-v4',render_mode='rgb_array')\n",
        "\n",
        "state_space = env.observation_space.shape\n",
        "action_space = env.action_space.n\n",
        "\n",
        "# Printing the results\n",
        "print(\"State Space Shape:\", state_space)\n",
        "print(\"Action Space Size:\", action_space)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNb93ywr5bTb",
        "outputId": "c033046e-bc2a-4a80-a63e-d789c02f6c89"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State Space Shape: (210, 160, 3)\n",
            "Action Space Size: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "batch_size = 32\n",
        "episodes = 10\n",
        "\n",
        "for episode in range(episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        # Preprocess state (if necessary)\n",
        "        # (No need for np.expand_dims)\n",
        "\n",
        "        # Choose action using epsilon-greedy policy\n",
        "        if np.random.rand() < agent.epsilon:\n",
        "            action = env.action_space.sample()  # Explore: choose a random action\n",
        "        else:\n",
        "            state_input = np.expand_dims(state, axis=0)  # Adding batch dimension\n",
        "            # action = np.argmax(agent.model.predict(state_input))\n",
        "            action = tf.convert_to_tensor(action) # Exploit: choose the best action\n",
        "\n",
        "        # action= self.env.action_space.sample()  # selects an action from a sample space randomly\n",
        "        # obs, reward, done, *info = self.env.step(action)\n",
        "\n",
        "\n",
        "        # Take the action in the environment\n",
        "        action= env.action_space.sample()\n",
        "        next_state, reward, done, *info = env.step(action)\n",
        "\n",
        "        # Preprocess next_state (if necessary)\n",
        "        # (No need for np.expand_dims)\n",
        "\n",
        "        # Store the experience in memory\n",
        "        agent.remember(state, action, reward, next_state, done)\n",
        "\n",
        "        # Update the state\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        # Apply epsilon decay\n",
        "        if agent.epsilon > agent.epsilon_min:\n",
        "            agent.epsilon *= agent.epsilon_decay\n",
        "\n",
        "        # Clear the previous plot and display the current frame\n",
        "        # clear_output(wait=True)\n",
        "        # plt.imshow(env.render())\n",
        "        # plt.show()\n",
        "\n",
        "        # Print episode statistics\n",
        "    print(f\"Episode: {episode+1}, Total Reward: {total_reward}, Epsilon: {agent.epsilon}\")\n",
        "\n",
        "# Close the environment after training\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2WLEzal6SRY",
        "outputId": "ba8879bd-50e2-45c1-f30f-7c58f1345684"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 1, Total Reward: 440.0, Epsilon: 0.00998645168764533\n",
            "Episode: 2, Total Reward: 125.0, Epsilon: 0.00998645168764533\n",
            "Episode: 3, Total Reward: 45.0, Epsilon: 0.00998645168764533\n",
            "Episode: 4, Total Reward: 70.0, Epsilon: 0.00998645168764533\n",
            "Episode: 5, Total Reward: 150.0, Epsilon: 0.00998645168764533\n",
            "Episode: 6, Total Reward: 160.0, Epsilon: 0.00998645168764533\n",
            "Episode: 7, Total Reward: 110.0, Epsilon: 0.00998645168764533\n",
            "Episode: 8, Total Reward: 110.0, Epsilon: 0.00998645168764533\n",
            "Episode: 9, Total Reward: 65.0, Epsilon: 0.00998645168764533\n",
            "Episode: 10, Total Reward: 120.0, Epsilon: 0.00998645168764533\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DQNAgent TRAINING ON PACMAN"
      ],
      "metadata": {
        "id": "K5djyCmgACSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam  # Import the Adam optimizer\n",
        "\n",
        "class DQN:\n",
        "    def __init__(self, state_space, action_space):\n",
        "        self.state_space = state_space\n",
        "        self.action_space = action_space\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.epsilon_min = 0.01\n",
        "        self.gamma = 0.99\n",
        "        self.learning_rate = 0.001\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(24, input_shape=self.state_space, activation='relu'))\n",
        "        model.add(Dense(24, activation='relu'))\n",
        "        model.add(Dense(self.action_space, activation='linear'))\n",
        "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))  # Use Adam optimizer\n",
        "        return model\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make('MsPacman-v0')\n",
        "\n",
        "# Get state and action space dimensions\n",
        "state_space = env.observation_space.shape\n",
        "action_space = env.action_space.n\n",
        "\n",
        "# Initialize DQN agent\n",
        "agent = DQN(state_space, action_space)\n"
      ],
      "metadata": {
        "id": "bEoRB6L4AAY3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create the MsPacman environment\n",
        "env = gym.make('MsPacman-v0')\n",
        "\n",
        "# Define the number of episodes\n",
        "episodes = 10\n",
        "\n",
        "for episode in range(episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        # Replace the following line with your specific algorithm to choose an action\n",
        "        action = env.action_space.sample()\n",
        "\n",
        "        # Take a step in the environment\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "\n",
        "    print(f\"Episode {episode + 1}, Total Reward: {total_reward}\")\n",
        "\n",
        "# Don't forget to close the environment after you finish using it\n",
        "env.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MfV4GLOAqFz",
        "outputId": "15423a83-7949-4c94-abf9-7b778f803297"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1, Total Reward: 190.0\n",
            "Episode 2, Total Reward: 190.0\n",
            "Episode 3, Total Reward: 380.0\n",
            "Episode 4, Total Reward: 270.0\n",
            "Episode 5, Total Reward: 100.0\n",
            "Episode 6, Total Reward: 250.0\n",
            "Episode 7, Total Reward: 260.0\n",
            "Episode 8, Total Reward: 180.0\n",
            "Episode 9, Total Reward: 120.0\n",
            "Episode 10, Total Reward: 250.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "agent.model.save('pacman_dqn_model.h5')"
      ],
      "metadata": {
        "id": "XqIB2SyjBO91"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import tensorflow as tf\n",
        "\n",
        "# Create the MsPacman environment\n",
        "env = gym.make('MsPacman-v0')\n",
        "\n",
        "# Load the trained model\n",
        "model = tf.keras.models.load_model('pacman_dqn_model.h5')\n",
        "\n",
        "# Define the number of episodes\n",
        "episodes = 20\n",
        "\n",
        "for episode in range(episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        # Replace the following line with your specific algorithm to choose an action using the trained model\n",
        "        # For example, in Deep Q-Network (DQN), you would use model.predict(state) to get the Q-values and select the action with the highest Q-value.\n",
        "        action = env.action_space.sample()\n",
        "\n",
        "        # Take a step in the environment\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "\n",
        "    print(f\"Episode {episode + 1}, Total Reward: {total_reward}\")\n",
        "\n",
        "# Don't forget to close the environment after you finish using it\n",
        "env.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDAr0HLDAqeB",
        "outputId": "96264e63-fd68-4ce2-d9a1-64966f557ad9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1, Total Reward: 170.0\n",
            "Episode 2, Total Reward: 340.0\n",
            "Episode 3, Total Reward: 190.0\n",
            "Episode 4, Total Reward: 130.0\n",
            "Episode 5, Total Reward: 250.0\n",
            "Episode 6, Total Reward: 330.0\n",
            "Episode 7, Total Reward: 240.0\n",
            "Episode 8, Total Reward: 200.0\n",
            "Episode 9, Total Reward: 240.0\n",
            "Episode 10, Total Reward: 230.0\n",
            "Episode 11, Total Reward: 200.0\n",
            "Episode 12, Total Reward: 260.0\n",
            "Episode 13, Total Reward: 200.0\n",
            "Episode 14, Total Reward: 240.0\n",
            "Episode 15, Total Reward: 200.0\n",
            "Episode 16, Total Reward: 300.0\n",
            "Episode 17, Total Reward: 170.0\n",
            "Episode 18, Total Reward: 140.0\n",
            "Episode 19, Total Reward: 250.0\n",
            "Episode 20, Total Reward: 230.0\n"
          ]
        }
      ]
    }
  ]
}